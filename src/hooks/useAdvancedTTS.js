import { useState, useRef, useCallback, useEffect } from 'react';

const useAdvancedTTS = () => {
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [ttsMode, setTtsMode] = useState('openai');
  const [isAudioContextInitialized, setIsAudioContextInitialized] = useState(false);
  const synthRef = useRef(null);
  const audioRef = useRef(null);
  const audioContextRef = useRef(null);

  const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);

  const initializeSpeech = useCallback(() => {
    if ('speechSynthesis' in window) {
      synthRef.current = window.speechSynthesis;
    }
  }, []);

  const initializeAudioContext = useCallback(() => {
    if (!isMobile || isAudioContextInitialized) return Promise.resolve();

    return new Promise((resolve) => {
      console.log('ğŸ“± ëª¨ë°”ì¼ ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹œì‘');
      
      const dummyAudio = new Audio();
      dummyAudio.src = 'data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQoGAACBhYqFbF1fdJivrJBhNjVgodDbq2EcBj+a2/LDciUFLIHO8tiJNwgZaLvt559NEAxQp+PwtmMcBjiR1/LMeSwFJHfH8N2QQAoUXrTp66hVFApGn+DyvmEaBSuq7f6lFhAKgQG5jdBgHwYqr';
      dummyAudio.volume = 0.01;
      
      dummyAudio.play().then(() => {
        console.log('âœ… ëª¨ë°”ì¼ ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ');
        setIsAudioContextInitialized(true);
        resolve();
      }).catch((error) => {
        console.warn('âš ï¸ ëª¨ë°”ì¼ ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
        setIsAudioContextInitialized(true);
        resolve();
      });
    });
  }, [isMobile, isAudioContextInitialized]);

  const speakWithWeb = useCallback((text) => {
    if (!synthRef.current) return Promise.resolve();

    return new Promise((resolve) => {
    try {
      synthRef.current.cancel();

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'ko-KR';
      utterance.rate = 1.1;
      utterance.pitch = 1.3;
      utterance.volume = 1.0;

      const voices = synthRef.current.getVoices();
      const koreanFemaleVoices = voices.filter(voice => 
        voice.lang.includes('ko') && 
        (voice.name.includes('Female') || voice.name.includes('ì—¬') || voice.name.includes('Yuna') || voice.name.includes('Heami'))
      );
      
      if (koreanFemaleVoices.length > 0) {
        const brightVoice = koreanFemaleVoices.find(voice => 
          voice.name.includes('Yuna') || voice.name.includes('Kyuri') || voice.name.includes('Heami')
        ) || koreanFemaleVoices[0];
        utterance.voice = brightVoice;
      }

      utterance.onstart = () => {
        setIsSpeaking(true);
        console.log('ì›¹ TTS ì‹œì‘:', text.substring(0, 30) + '...');
      };

      utterance.onend = () => {
        setIsSpeaking(false);
        console.log('ì›¹ TTS ì™„ë£Œ');
          resolve();
      };

      utterance.onerror = () => {
        setIsSpeaking(false);
        console.error('ì›¹ TTS ì˜¤ë¥˜');
          resolve();
      };

      synthRef.current.speak(utterance);
    } catch (error) {
      console.error('ì›¹ TTS ì˜¤ë¥˜:', error);
      setIsSpeaking(false);
        resolve();
    }
    });
  }, []);

  const speakWithOpenAI = useCallback(async (text) => {
    const apiKey = import.meta.env.VITE_OPENAI_API_KEY;
    if (!apiKey || !apiKey.startsWith('sk-')) {
      console.warn('OpenAI API í‚¤ê°€ ì—†ì–´ì„œ ì›¹ TTSë¡œ ëŒ€ì²´');
      return await speakWithWeb(text);
    }

    if (isMobile && !isAudioContextInitialized) {
      console.warn('ğŸ“± ëª¨ë°”ì¼ì—ì„œ ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ì„œ ì›¹ TTSë¡œ ëŒ€ì²´');
      return await speakWithWeb(text);
    }

    try {
      setIsSpeaking(true);
      console.log('OpenAI TTS í˜¸ì¶œ ì‹œì‘:', text.substring(0, 30) + '...');

      const response = await fetch('https://api.openai.com/v1/audio/speech', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'tts-1',
          input: text,
          voice: 'nova',
          speed: 1.2
        }),
      });

      if (!response.ok) {
        throw new Error(`OpenAI TTS API ì˜¤ë¥˜: ${response.status}`);
      }

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
      
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }

      audioRef.current = new Audio(audioUrl);
      
      if (isMobile) {
        audioRef.current.preload = 'auto';
        audioRef.current.volume = 1.0;
      }
      
      return new Promise((resolve, reject) => {
      audioRef.current.onended = () => {
        setIsSpeaking(false);
        URL.revokeObjectURL(audioUrl);
        console.log('OpenAI TTS ì¬ìƒ ì™„ë£Œ');
          resolve();
      };

      audioRef.current.onerror = () => {
        setIsSpeaking(false);
        URL.revokeObjectURL(audioUrl);
        console.error('OpenAI TTS ì¬ìƒ ì˜¤ë¥˜');
          resolve();
      };

        audioRef.current.play().then(() => {
      console.log('OpenAI TTS ì¬ìƒ ì‹œì‘');
        }).catch((playError) => {
          console.error('OpenAI TTS ì¬ìƒ ì‹œì‘ ì˜¤ë¥˜:', playError);
          setIsSpeaking(false);
          URL.revokeObjectURL(audioUrl);
          
          if (isMobile) {
            console.warn('ğŸ“± ëª¨ë°”ì¼ì—ì„œ OpenAI TTS ì¬ìƒ ì‹¤íŒ¨, ì›¹ TTSë¡œ ëŒ€ì²´');
            speakWithWeb(text).then(resolve).catch(resolve);
          } else {
            resolve();
          }
        });
      });

    } catch (error) {
      console.error('OpenAI TTS ì˜¤ë¥˜:', error);
      setIsSpeaking(false);
      return await speakWithWeb(text);
    }
  }, [speakWithWeb, isMobile, isAudioContextInitialized]);

  const speakWithTTSMaker = useCallback(async (text) => {
    const apiKey = import.meta.env.VITE_TTSMAKER_API_KEY;
    if (!apiKey || apiKey === 'your_ttsmaker_api_key_here') {
      console.warn('TTSMaker API í‚¤ê°€ ì—†ì–´ì„œ OpenAI TTSë¡œ ëŒ€ì²´');
      return await speakWithOpenAI(text);
    }

    try {
      setIsSpeaking(true);
      console.log('TTSMaker TTS í˜¸ì¶œ ì‹œì‘:', text.substring(0, 30) + '...');

      const response = await fetch('https://api.ttsmaker.com/v1/speech', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify({
          text,
          voice: 'ko-KR-female',
          format: 'mp3',
          speed: 1.1
        })
      });

      if (!response.ok) {
        throw new Error(`TTSMaker API ì˜¤ë¥˜: ${response.status}`);
      }

      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);

      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }

      audioRef.current = new Audio(audioUrl);
      
      return new Promise((resolve, reject) => {
      audioRef.current.onended = () => {
        setIsSpeaking(false);
        URL.revokeObjectURL(audioUrl);
        console.log('TTSMaker TTS ì¬ìƒ ì™„ë£Œ');
          resolve();
      };

      audioRef.current.onerror = () => {
        setIsSpeaking(false);
        URL.revokeObjectURL(audioUrl);
        console.error('TTSMaker TTS ì¬ìƒ ì˜¤ë¥˜');
          resolve();
      };

        audioRef.current.play().then(() => {
      console.log('TTSMaker TTS ì¬ìƒ ì‹œì‘');
        }).catch((playError) => {
          console.error('TTSMaker TTS ì¬ìƒ ì‹œì‘ ì˜¤ë¥˜:', playError);
          setIsSpeaking(false);
          URL.revokeObjectURL(audioUrl);
          resolve();
        });
      });

    } catch (error) {
      console.error('TTSMaker TTS ì˜¤ë¥˜:', error);
      setIsSpeaking(false);
      return await speakWithOpenAI(text);
    }
  }, [speakWithOpenAI]);

  const speakText = useCallback(async (text) => {
    if (!text) return Promise.resolve();

    if (isMobile && !isAudioContextInitialized) {
      console.log('ğŸ“± ëª¨ë°”ì¼ì—ì„œ ì²« ë²ˆì§¸ TTS í˜¸ì¶œ - ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹œë„');
      await initializeAudioContext();
    }

    console.log(`TTS ëª¨ë“œ: ${ttsMode}, í…ìŠ¤íŠ¸: ${text.substring(0, 50)}...`);

    switch (ttsMode) {
      case 'ttsmaker':
        return await speakWithTTSMaker(text);
      case 'openai':
        return await speakWithOpenAI(text);
      case 'web':
      default:
        return await speakWithWeb(text);
    }
  }, [ttsMode, speakWithTTSMaker, speakWithOpenAI, speakWithWeb, isMobile, isAudioContextInitialized, initializeAudioContext]);

  const stopSpeaking = useCallback(() => {
    if (synthRef.current) {
      synthRef.current.cancel();
    }
    
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current = null;
    }
    
    setIsSpeaking(false);
    console.log('TTS ì¤‘ì§€ë¨');
  }, []);

  const switchTTSMode = useCallback((mode) => {
    stopSpeaking();
    setTtsMode(mode);
    console.log(`TTS ëª¨ë“œ ë³€ê²½: ${mode}`);
  }, [stopSpeaking]);

  const getAvailableModes = useCallback(() => {
    const modes = [
      { 
        id: 'web', 
        name: 'ì›¹ TTS', 
        icon: 'ğŸŒ', 
        available: true,
        description: 'ë¸Œë¼ìš°ì € ë‚´ì¥ ìŒì„±'
      }
    ];

    if (import.meta.env.VITE_OPENAI_API_KEY?.startsWith('sk-')) {
      modes.push({
        id: 'openai',
        name: 'OpenAI TTS',
        icon: 'ğŸ¤–',
        available: true,
        description: 'AI ê³ í’ˆì§ˆ ìŒì„±'
      });
    }

    if (import.meta.env.VITE_TTSMAKER_API_KEY && import.meta.env.VITE_TTSMAKER_API_KEY !== 'your_ttsmaker_api_key_here') {
      modes.push({
        id: 'ttsmaker',
        name: 'TTSMaker',
        icon: 'ğŸ™ï¸',
        available: true,
        description: 'ì „ë¬¸ TTS ì„œë¹„ìŠ¤'
      });
    }

    return modes;
  }, []);

  return {
    isSpeaking,
    ttsMode,
    initializeSpeech,
    speakText,
    stopSpeaking,
    switchTTSMode,
    getAvailableModes,
    initializeAudioContext,
    isAudioContextInitialized
  };
};

export default useAdvancedTTS; 